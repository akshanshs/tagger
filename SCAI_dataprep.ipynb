{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uQqA8W9RMxsU"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_questions = \"/content/drive/MyDrive/nlp/classification/data/questions.csv\"\n",
        "file_tags = \"/content/drive/MyDrive/nlp/classification/data/tags.csv\"\n",
        "encodings = [\"utf-8\", \"latin1\", \"utf-16\"]"
      ],
      "metadata": {
        "id": "dt0vYNMaNAkC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "for encode in encodings:\n",
        "  try:\n",
        "    dfq = pd.read_csv(file_questions, encoding = encode)\n",
        "    print(\"File reading succesful with \", encode)\n",
        "    break\n",
        "  except UnicodeDecodeError:\n",
        "    continue\n",
        "\n",
        "dfq.shape"
      ],
      "metadata": {
        "id": "dp9QebQyNKJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "for encode in encodings:\n",
        "  try:\n",
        "    dft = pd.read_csv(file_tags, encoding = encode)\n",
        "    print(\"File reading succesful with \", encode)\n",
        "    break\n",
        "  except UnicodeDecodeError:\n",
        "    continue\n",
        "\n",
        "dft.shape"
      ],
      "metadata": {
        "id": "61b2sY6vNKHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dfq.merge(dft, on = 'Id', how='left')\n",
        "df.dropna(subset=['Tag'], inplace = True)"
      ],
      "metadata": {
        "id": "Pkiktl-JNKES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(1000)"
      ],
      "metadata": {
        "id": "DUWVq3jdONMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "0SAfgxZxNbIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "metadata": {
        "id": "-SnzTupHNKBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "TCCOO-w3NJ-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0kLoey0NJ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "def clean_text(text):\n",
        "\n",
        "  text = re.sub(r'\\d', ' ', text)\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', ' ', text)   # Remove non-alphanumeric characters and hyphens\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word.lower() not in stop_words] #stop words\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]  # lemmatization\n",
        "  #print(tokens)\n",
        "  #print(' '.join(tokens))\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "3MwiWerQNlKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tag'] = df['Tag'].astype(str)\n",
        "df['clean_tag'] = df['clean_tag'].apply(clean_text)\n",
        "df['clean_tag'] = df['clean_tag'].apply(lambda tags: tags.split(' ')[0] if pd.notna(tags) else ' ')\n",
        "df['clean_tag'] = df['clean_tag'].str.strip()"
      ],
      "metadata": {
        "id": "jyp_RTqVNlHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_tags = df['clean_tag'].value_counts().nlargest(20).index.tolist()\n",
        "df = df[df['clean_tag'].isin(top_tags)]"
      ],
      "metadata": {
        "id": "O1v9FB_6NlET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_title'] = df['Title'].astype(str)\n",
        "df['clean_title'] = df['clean_title'].apply(clean_text)"
      ],
      "metadata": {
        "id": "yp-XBwqoNlBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_body'] = df['Body'].astype(str)\n",
        "df['clean_body'] = df['clean_body'].apply(clean_text)"
      ],
      "metadata": {
        "id": "-OrJTsnQNk-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Id', 'OwnerUserId', 'CreationDate', 'Score', 'Title', 'Body', 'Tag'])\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "f70TeSnoNk8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "t1zwRzBeOCgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df_clean.csv', index = False)\n",
        "from google.colab import files\n",
        "files.download('df_clean.csv')"
      ],
      "metadata": {
        "id": "CimAosXINk5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmG97LvbNk2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQQqhyisNkzY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}