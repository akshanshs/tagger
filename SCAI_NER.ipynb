{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# spacy model"
      ],
      "metadata": {
        "id": "WjmmoayYO0na"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d66mI6m2OXTd"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.tokens import Doc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_df = \"/content/drive/MyDrive/nlp/classification/df_clean.csv\"\n",
        "df = pd.read_csv(file_df)\n",
        "df['clean_body'] = df['clean_body'].astype(str)\n",
        "df['clean_title'] = df['clean_title'].astype(str)\n",
        "df['clean_tag'] = df['clean_tag'].astype(str)\n",
        "\n",
        "df_g = df.groupby(['clean_title', 'clean_body'])['clean_tag'].apply(lambda x: ','.join(x)).reset_index()\n",
        "df_g['clean_tag'] = df_g['clean_tag'].apply(lambda tags: ','.join(set(tags.split(','))))\n",
        "\n",
        "all_tags = ','.join(df_g['clean_tag']).split(',')\n",
        "tag_counts = pd.Series(all_tags).value_counts()\n",
        "top_2_tags = tag_counts.nlargest(2).index.to_list()\n",
        "def select_top_2_tags(tags):\n",
        "  tag_list = tags.split(',')\n",
        "  top_2_tags_entry = sorted(tag_list, key = lambda x: tag_counts.get(x, 0), reverse = True)[:2]\n",
        "  return ','.join(top_2_tags_entry)\n",
        "\n",
        "df_g['clean_tag'] = df_g['clean_tag'].apply(select_top_2_tags)"
      ],
      "metadata": {
        "id": "Ca8HsaTzOfO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_g.to_csv('df_grp_2tag.csv', index = False)\n",
        "# from google.colab import files\n",
        "# files.download('df_grp_2tag.csv')"
      ],
      "metadata": {
        "id": "RKB0WulqPJ8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_g = df_g.head(1000)\n",
        "df_g.head()"
      ],
      "metadata": {
        "id": "b9S1ChgOPFAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "ner = nlp.get_pipe('ner')\n",
        "ner.add_label('tech_stack')"
      ],
      "metadata": {
        "id": "UNNrAAgcPP_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_example(row):\n",
        "    text = row['clean_title'] + ' ' + row['clean_body']\n",
        "    tags = row['clean_tag'].split(',')\n",
        "    entities = []\n",
        "\n",
        "    for tag in tags:\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            start = text.find(tag, start)\n",
        "            if start == -1:\n",
        "                break\n",
        "            end = start + len(tag)\n",
        "            entities.append((start, end, 'tech_stack'))\n",
        "            start = end  # Move start to the end of the current tag to avoid overlapping\n",
        "\n",
        "    doc = nlp.make_doc(text)\n",
        "    spans = [doc.char_span(start, end, label=label) for start, end, label in entities]\n",
        "    spans = [span for span in spans if span is not None]  # Remove None values (invalid spans)\n",
        "    doc.ents = spans\n",
        "\n",
        "    example = Example.from_dict(doc, {\"entities\": [(span.start_char, span.end_char, span.label_) for span in doc.ents]})\n",
        "    return example"
      ],
      "metadata": {
        "id": "OqAaKsYpPP81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "\n",
        "for irow, row in df_g.iterrows():\n",
        "    example = create_example(row)\n",
        "    train_data.append(example)"
      ],
      "metadata": {
        "id": "gqjJcJhpPP6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.create_optimizer()\n",
        "\n",
        "for epoch in range(1):\n",
        "  losses = {}\n",
        "  print('epoch')\n",
        "  random.shuffle(train_data)\n",
        "  for batch in spacy.util.minibatch(train_data, size = 2):\n",
        "    print(losses)\n",
        "    for example in batch:\n",
        "      nlp.update([example], drop = 0.5, losses=losses)"
      ],
      "metadata": {
        "id": "22HAKgebPP3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk('tuned_spacy_ner')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('tuned_spacy_ner')"
      ],
      "metadata": {
        "id": "8v2Hb-zGPP0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions"
      ],
      "metadata": {
        "id": "CE-wgwdpPPxJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## code for api\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "json_data = request.get_json()\n",
        "\n",
        "        new_text1 = json_data['heading']\n",
        "        new_text2 = json_data['description']\n",
        "combined_text = new_text1 + ' ' + new_text2\n",
        "\n",
        "nlp_fine_tuned = spacy.load(\"fine_tuned_ner_model\")\n",
        "doc = nlp_fine_tuned(combined_text)\n",
        "\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}\")"
      ],
      "metadata": {
        "id": "te7Cn2s8PPro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XujLdWZPPox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build LLM model"
      ],
      "metadata": {
        "id": "IjkGwHCPQvRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForTokenClassification, DistilBertConfig, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qsAOmzw2PPlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
        "\n",
        "# Add custom NER labels to the model\n",
        "labels = ['O', 'NER']\n",
        "model.config.id2label = {i: label for i, label in enumerate(labels)}\n",
        "model.config.label2id = {label: i for i, label in enumerate(labels)}"
      ],
      "metadata": {
        "id": "yOACwb2aQ7bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, tags):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        labels = [0] * len(encoding['input_ids'][0])\n",
        "\n",
        "        # Convert tag indices to character positions\n",
        "        for tag in self.tags[idx].split(','):\n",
        "            start_idx = self.texts[idx].find(tag)\n",
        "            end_idx = start_idx + len(tag)\n",
        "            # Convert character positions to token indices\n",
        "            start_token, end_token = encoding.char_to_token(start_idx, end_idx)\n",
        "            if start_token is not None and end_token is not None:\n",
        "                labels[start_token:end_token] = [1] * (end_token - start_token)\n",
        "\n",
        "        labels = torch.tensor(labels).unsqueeze(0)  # Batch size of 1\n",
        "        return {'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': labels}"
      ],
      "metadata": {
        "id": "sKTcRAj8Q7Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NERDataset(df['clean_title'] + ' ' + df['clean_body'], df['clean_tag'])\n",
        "\n",
        "# Fine-tune the model\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "APJS3rweQ7WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(1):\n",
        "    for batch in tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].squeeze(1)\n",
        "        attention_mask = batch['attention_mask'].squeeze(1)\n",
        "        labels = batch['labels']\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('tuned_dbert_ner')"
      ],
      "metadata": {
        "id": "cVtfccAyQ7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_HLM-BYQ7QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8IWyYiGQ7NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQkkuJwJQ7KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUkYvlD7Q7HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9_l1urgQ7E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Om2K6oVQ7CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBxGhN0cQ6-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}